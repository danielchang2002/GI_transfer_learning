{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In google colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_3Nqa3VXZBoB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bz18HyNjZL9u",
    "outputId": "4d5eb169-844e-46de-d080-6f3ac8e6a436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  normalized_no_leakage.zip\n",
      "  inflating: depmap_crispr_zscore.csv  \n",
      "  inflating: depmap_expression_lfc_zscore.csv  \n",
      "  inflating: hap1_expression_lfc.csv  \n",
      "  inflating: hap1_crispr.csv         \n"
     ]
    }
   ],
   "source": [
    "! cp /content/drive/MyDrive/gin_depmap_transfer_learning/normalized_no_leakage.zip .\n",
    "! unzip normalized_no_leakage.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zY2ee3RhZPKH",
    "outputId": "0ba0c6ed-1cb1-4315-ed2f-ea4e24b89813"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60, 16372), (60, 16432), (1021, 16372), (1021, 16432))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dir = \"\"\n",
    "\n",
    "hap1_expression_lfc = pd.read_csv(norm_dir + \"hap1_expression_lfc.csv\", index_col=0)\n",
    "hap1_crispr = pd.read_csv(norm_dir + \"hap1_crispr.csv\", index_col=0)\n",
    "depmap_expression_lfc_zscore = pd.read_csv(norm_dir + \"depmap_expression_lfc_zscore.csv\", index_col=0)\n",
    "depmap_crispr_zscore = pd.read_csv(norm_dir + \"depmap_crispr_zscore.csv\", index_col=0)\n",
    "hap1_expression_lfc.shape, hap1_crispr.shape, depmap_expression_lfc_zscore.shape, depmap_crispr_zscore.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qaci00G1OnT",
    "outputId": "d4cff4bc-e97c-4951-e075-7864b663170b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  tcga_expression_lfc_zscore.csv.zip\n",
      "  inflating: tcga_expression_lfc_zscore.csv  \n"
     ]
    }
   ],
   "source": [
    "! cp /content/drive/MyDrive/gin_depmap_transfer_learning/tcga_expression_lfc_zscore.csv.zip .\n",
    "! unzip tcga_expression_lfc_zscore.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gp9LS_7xZQeb"
   },
   "outputs": [],
   "source": [
    "tcga_expression_lfc_zscore = pd.read_csv(norm_dir + \"tcga_expression_lfc_zscore.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTCeEkyWZTCb",
    "outputId": "ac4bbd7d-a418-451e-8f8b-df463c2edd90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10534, 16372)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcga_expression_lfc_zscore.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghVQ4trohIte",
    "outputId": "054244d4-cfcd-414c-9b31-5fda64b2dc99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GI_transfer_learning'...\n",
      "remote: Enumerating objects: 9, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 9 (delta 0), reused 9 (delta 0), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (9/9), done.\n",
      "/content/GI_transfer_learning/src\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/danielchang2002/GI_transfer_learning\n",
    "%cd GI_transfer_learning/src\n",
    "from utils import *\n",
    "from vae import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GSpQQBeg7xNm"
   },
   "outputs": [],
   "source": [
    "from vae import loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1XfNyMEvl5RZ"
   },
   "outputs": [],
   "source": [
    "hap1_expression_lfc_zscore = zscore(hap1_expression_lfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zfY26l9DdHsD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def get_dataloader(df, shuffle=True):\n",
    "    dataset = TensorDataset(torch.tensor(df.values).float().cuda())\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = get_dataloader(pd.concat([tcga_expression_lfc_zscore, depmap_expression_lfc_zscore, hap1_expression_lfc_zscore]), shuffle=True)\n",
    "test_loader = get_dataloader(hap1_expression_lfc_zscore, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oEZO4Zk2dHuB"
   },
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "K0y6UzxxdHwM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IcBmlgjEH1nV"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zhbnRCvHsmDG"
   },
   "outputs": [],
   "source": [
    "model = VAE(depmap_expression_lfc_zscore.shape[1], 4096, 128).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 300\n",
    "num_warmup_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sskJUiS6c6UE",
    "outputId": "b34cd761-39de-4a2f-96d4-88247794a8b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTrain loss: 13521.797 \t Test loss: 17751.369\n",
      "Epoch: 0 \tTrain mse: 0.825 \t Test mse: 1.084\n",
      "Epoch: 1 \tTrain loss: 10061.823 \t Test loss: 17238.431\n",
      "Epoch: 1 \tTrain mse: 0.615 \t Test mse: 1.053\n",
      "Epoch: 2 \tTrain loss: 9256.011 \t Test loss: 16827.140\n",
      "Epoch: 2 \tTrain mse: 0.565 \t Test mse: 1.028\n",
      "Epoch: 3 \tTrain loss: 8730.225 \t Test loss: 16465.939\n",
      "Epoch: 3 \tTrain mse: 0.533 \t Test mse: 1.006\n",
      "Epoch: 4 \tTrain loss: 8326.342 \t Test loss: 15916.825\n",
      "Epoch: 4 \tTrain mse: 0.508 \t Test mse: 0.972\n",
      "Epoch: 5 \tTrain loss: 8022.223 \t Test loss: 15613.763\n",
      "Epoch: 5 \tTrain mse: 0.490 \t Test mse: 0.954\n",
      "Epoch: 6 \tTrain loss: 7713.692 \t Test loss: 15383.811\n",
      "Epoch: 6 \tTrain mse: 0.471 \t Test mse: 0.940\n",
      "Epoch: 7 \tTrain loss: 7475.024 \t Test loss: 14953.206\n",
      "Epoch: 7 \tTrain mse: 0.456 \t Test mse: 0.913\n",
      "Epoch: 8 \tTrain loss: 7274.270 \t Test loss: 14782.945\n",
      "Epoch: 8 \tTrain mse: 0.444 \t Test mse: 0.903\n",
      "Epoch: 9 \tTrain loss: 7088.612 \t Test loss: 14517.009\n",
      "Epoch: 9 \tTrain mse: 0.432 \t Test mse: 0.886\n",
      "Epoch: 10 \tTrain loss: 6901.514 \t Test loss: 14273.318\n",
      "Epoch: 10 \tTrain mse: 0.421 \t Test mse: 0.872\n",
      "Epoch: 11 \tTrain loss: 6747.284 \t Test loss: 13940.291\n",
      "Epoch: 11 \tTrain mse: 0.411 \t Test mse: 0.851\n",
      "Epoch: 12 \tTrain loss: 6611.915 \t Test loss: 13712.873\n",
      "Epoch: 12 \tTrain mse: 0.403 \t Test mse: 0.837\n",
      "Epoch: 13 \tTrain loss: 6477.801 \t Test loss: 13619.211\n",
      "Epoch: 13 \tTrain mse: 0.395 \t Test mse: 0.831\n",
      "Epoch: 14 \tTrain loss: 6376.826 \t Test loss: 13303.073\n",
      "Epoch: 14 \tTrain mse: 0.389 \t Test mse: 0.812\n",
      "Epoch: 15 \tTrain loss: 6265.556 \t Test loss: 13169.944\n",
      "Epoch: 15 \tTrain mse: 0.382 \t Test mse: 0.804\n",
      "Epoch: 16 \tTrain loss: 6167.339 \t Test loss: 12975.379\n",
      "Epoch: 16 \tTrain mse: 0.376 \t Test mse: 0.792\n",
      "Epoch: 17 \tTrain loss: 6080.489 \t Test loss: 12713.875\n",
      "Epoch: 17 \tTrain mse: 0.370 \t Test mse: 0.776\n",
      "Epoch: 18 \tTrain loss: 5995.297 \t Test loss: 12575.268\n",
      "Epoch: 18 \tTrain mse: 0.365 \t Test mse: 0.767\n",
      "Epoch: 19 \tTrain loss: 5916.014 \t Test loss: 12516.448\n",
      "Epoch: 19 \tTrain mse: 0.360 \t Test mse: 0.763\n",
      "Epoch: 20 \tTrain loss: 5832.329 \t Test loss: 12175.764\n",
      "Epoch: 20 \tTrain mse: 0.355 \t Test mse: 0.742\n",
      "Epoch: 21 \tTrain loss: 5753.268 \t Test loss: 12027.741\n",
      "Epoch: 21 \tTrain mse: 0.350 \t Test mse: 0.733\n",
      "Epoch: 22 \tTrain loss: 5681.064 \t Test loss: 11934.876\n",
      "Epoch: 22 \tTrain mse: 0.346 \t Test mse: 0.728\n",
      "Epoch: 23 \tTrain loss: 5630.189 \t Test loss: 11711.033\n",
      "Epoch: 23 \tTrain mse: 0.342 \t Test mse: 0.714\n",
      "Epoch: 24 \tTrain loss: 5585.854 \t Test loss: 11607.145\n",
      "Epoch: 24 \tTrain mse: 0.339 \t Test mse: 0.707\n",
      "Epoch: 25 \tTrain loss: 5503.006 \t Test loss: 11533.649\n",
      "Epoch: 25 \tTrain mse: 0.334 \t Test mse: 0.703\n",
      "Epoch: 26 \tTrain loss: 5464.943 \t Test loss: 11376.062\n",
      "Epoch: 26 \tTrain mse: 0.332 \t Test mse: 0.693\n",
      "Epoch: 27 \tTrain loss: 5376.289 \t Test loss: 11181.011\n",
      "Epoch: 27 \tTrain mse: 0.326 \t Test mse: 0.681\n",
      "Epoch: 28 \tTrain loss: 5330.755 \t Test loss: 11049.132\n",
      "Epoch: 28 \tTrain mse: 0.323 \t Test mse: 0.673\n",
      "Epoch: 29 \tTrain loss: 5295.637 \t Test loss: 10968.991\n",
      "Epoch: 29 \tTrain mse: 0.321 \t Test mse: 0.667\n",
      "Epoch: 30 \tTrain loss: 5235.440 \t Test loss: 10758.283\n",
      "Epoch: 30 \tTrain mse: 0.317 \t Test mse: 0.654\n",
      "Epoch: 31 \tTrain loss: 5189.942 \t Test loss: 10618.386\n",
      "Epoch: 31 \tTrain mse: 0.314 \t Test mse: 0.646\n",
      "Epoch: 32 \tTrain loss: 5145.279 \t Test loss: 10438.180\n",
      "Epoch: 32 \tTrain mse: 0.311 \t Test mse: 0.634\n",
      "Epoch: 33 \tTrain loss: 5084.451 \t Test loss: 10352.505\n",
      "Epoch: 33 \tTrain mse: 0.308 \t Test mse: 0.629\n",
      "Epoch: 34 \tTrain loss: 5042.978 \t Test loss: 10262.826\n",
      "Epoch: 34 \tTrain mse: 0.305 \t Test mse: 0.623\n",
      "Epoch: 35 \tTrain loss: 4986.354 \t Test loss: 10113.630\n",
      "Epoch: 35 \tTrain mse: 0.301 \t Test mse: 0.614\n",
      "Epoch: 36 \tTrain loss: 4956.797 \t Test loss: 9956.775\n",
      "Epoch: 36 \tTrain mse: 0.299 \t Test mse: 0.604\n",
      "Epoch: 37 \tTrain loss: 4909.332 \t Test loss: 9786.668\n",
      "Epoch: 37 \tTrain mse: 0.296 \t Test mse: 0.593\n",
      "Epoch: 38 \tTrain loss: 4884.802 \t Test loss: 9654.708\n",
      "Epoch: 38 \tTrain mse: 0.295 \t Test mse: 0.585\n",
      "Epoch: 39 \tTrain loss: 4835.655 \t Test loss: 9506.674\n",
      "Epoch: 39 \tTrain mse: 0.291 \t Test mse: 0.576\n",
      "Epoch: 40 \tTrain loss: 4812.571 \t Test loss: 9409.703\n",
      "Epoch: 40 \tTrain mse: 0.290 \t Test mse: 0.570\n",
      "Epoch: 41 \tTrain loss: 4746.878 \t Test loss: 9154.856\n",
      "Epoch: 41 \tTrain mse: 0.286 \t Test mse: 0.554\n",
      "Epoch: 42 \tTrain loss: 4736.193 \t Test loss: 9141.876\n",
      "Epoch: 42 \tTrain mse: 0.285 \t Test mse: 0.553\n",
      "Epoch: 43 \tTrain loss: 4701.418 \t Test loss: 8991.805\n",
      "Epoch: 43 \tTrain mse: 0.283 \t Test mse: 0.543\n",
      "Epoch: 44 \tTrain loss: 4663.477 \t Test loss: 8908.773\n",
      "Epoch: 44 \tTrain mse: 0.280 \t Test mse: 0.538\n",
      "Epoch: 45 \tTrain loss: 4609.395 \t Test loss: 8741.257\n",
      "Epoch: 45 \tTrain mse: 0.277 \t Test mse: 0.527\n",
      "Epoch: 46 \tTrain loss: 4588.206 \t Test loss: 8572.130\n",
      "Epoch: 46 \tTrain mse: 0.275 \t Test mse: 0.516\n",
      "Epoch: 47 \tTrain loss: 4556.204 \t Test loss: 8502.682\n",
      "Epoch: 47 \tTrain mse: 0.273 \t Test mse: 0.512\n",
      "Epoch: 48 \tTrain loss: 4525.686 \t Test loss: 8353.584\n",
      "Epoch: 48 \tTrain mse: 0.271 \t Test mse: 0.502\n",
      "Epoch: 49 \tTrain loss: 4499.390 \t Test loss: 8224.218\n",
      "Epoch: 49 \tTrain mse: 0.269 \t Test mse: 0.494\n",
      "Epoch: 50 \tTrain loss: 4471.321 \t Test loss: 8096.486\n",
      "Epoch: 50 \tTrain mse: 0.267 \t Test mse: 0.486\n",
      "Epoch: 51 \tTrain loss: 4436.540 \t Test loss: 7980.105\n",
      "Epoch: 51 \tTrain mse: 0.265 \t Test mse: 0.478\n",
      "Epoch: 52 \tTrain loss: 4411.887 \t Test loss: 7848.435\n",
      "Epoch: 52 \tTrain mse: 0.263 \t Test mse: 0.470\n",
      "Epoch: 53 \tTrain loss: 4409.561 \t Test loss: 7815.227\n",
      "Epoch: 53 \tTrain mse: 0.263 \t Test mse: 0.468\n",
      "Epoch: 54 \tTrain loss: 4363.747 \t Test loss: 7693.503\n",
      "Epoch: 54 \tTrain mse: 0.260 \t Test mse: 0.459\n",
      "Epoch: 55 \tTrain loss: 4359.950 \t Test loss: 7623.729\n",
      "Epoch: 55 \tTrain mse: 0.259 \t Test mse: 0.455\n",
      "Epoch: 56 \tTrain loss: 4320.120 \t Test loss: 7524.745\n",
      "Epoch: 56 \tTrain mse: 0.257 \t Test mse: 0.449\n",
      "Epoch: 57 \tTrain loss: 4313.823 \t Test loss: 7477.724\n",
      "Epoch: 57 \tTrain mse: 0.256 \t Test mse: 0.445\n",
      "Epoch: 58 \tTrain loss: 4275.584 \t Test loss: 7353.040\n",
      "Epoch: 58 \tTrain mse: 0.254 \t Test mse: 0.437\n",
      "Epoch: 59 \tTrain loss: 4255.198 \t Test loss: 7197.084\n",
      "Epoch: 59 \tTrain mse: 0.252 \t Test mse: 0.427\n",
      "Epoch: 60 \tTrain loss: 4238.063 \t Test loss: 7119.151\n",
      "Epoch: 60 \tTrain mse: 0.251 \t Test mse: 0.422\n",
      "Epoch: 61 \tTrain loss: 4212.722 \t Test loss: 7027.322\n",
      "Epoch: 61 \tTrain mse: 0.249 \t Test mse: 0.416\n",
      "Epoch: 62 \tTrain loss: 4211.513 \t Test loss: 6951.646\n",
      "Epoch: 62 \tTrain mse: 0.249 \t Test mse: 0.411\n",
      "Epoch: 63 \tTrain loss: 4183.138 \t Test loss: 6845.283\n",
      "Epoch: 63 \tTrain mse: 0.247 \t Test mse: 0.403\n",
      "Epoch: 64 \tTrain loss: 4164.417 \t Test loss: 6772.511\n",
      "Epoch: 64 \tTrain mse: 0.245 \t Test mse: 0.399\n",
      "Epoch: 65 \tTrain loss: 4142.848 \t Test loss: 6736.116\n",
      "Epoch: 65 \tTrain mse: 0.244 \t Test mse: 0.396\n",
      "Epoch: 66 \tTrain loss: 4127.121 \t Test loss: 6629.870\n",
      "Epoch: 66 \tTrain mse: 0.243 \t Test mse: 0.389\n",
      "Epoch: 67 \tTrain loss: 4130.666 \t Test loss: 6559.649\n",
      "Epoch: 67 \tTrain mse: 0.243 \t Test mse: 0.384\n",
      "Epoch: 68 \tTrain loss: 4094.038 \t Test loss: 6490.251\n",
      "Epoch: 68 \tTrain mse: 0.240 \t Test mse: 0.380\n",
      "Epoch: 69 \tTrain loss: 4063.146 \t Test loss: 6355.742\n",
      "Epoch: 69 \tTrain mse: 0.238 \t Test mse: 0.371\n",
      "Epoch: 70 \tTrain loss: 4066.021 \t Test loss: 6286.381\n",
      "Epoch: 70 \tTrain mse: 0.238 \t Test mse: 0.366\n",
      "Epoch: 71 \tTrain loss: 4061.111 \t Test loss: 6269.129\n",
      "Epoch: 71 \tTrain mse: 0.237 \t Test mse: 0.364\n",
      "Epoch: 72 \tTrain loss: 4026.914 \t Test loss: 6197.247\n",
      "Epoch: 72 \tTrain mse: 0.235 \t Test mse: 0.359\n",
      "Epoch: 73 \tTrain loss: 4022.618 \t Test loss: 6146.618\n",
      "Epoch: 73 \tTrain mse: 0.234 \t Test mse: 0.356\n",
      "Epoch: 74 \tTrain loss: 3999.807 \t Test loss: 6083.426\n",
      "Epoch: 74 \tTrain mse: 0.233 \t Test mse: 0.351\n",
      "Epoch: 75 \tTrain loss: 3995.084 \t Test loss: 6012.316\n",
      "Epoch: 75 \tTrain mse: 0.232 \t Test mse: 0.347\n",
      "Epoch: 76 \tTrain loss: 3970.108 \t Test loss: 5991.424\n",
      "Epoch: 76 \tTrain mse: 0.230 \t Test mse: 0.345\n",
      "Epoch: 77 \tTrain loss: 3958.820 \t Test loss: 5911.358\n",
      "Epoch: 77 \tTrain mse: 0.230 \t Test mse: 0.339\n",
      "Epoch: 78 \tTrain loss: 3942.814 \t Test loss: 5897.135\n",
      "Epoch: 78 \tTrain mse: 0.228 \t Test mse: 0.338\n",
      "Epoch: 79 \tTrain loss: 3934.367 \t Test loss: 5799.815\n",
      "Epoch: 79 \tTrain mse: 0.227 \t Test mse: 0.331\n",
      "Epoch: 80 \tTrain loss: 3935.429 \t Test loss: 5761.719\n",
      "Epoch: 80 \tTrain mse: 0.227 \t Test mse: 0.328\n",
      "Epoch: 81 \tTrain loss: 3898.704 \t Test loss: 5677.493\n",
      "Epoch: 81 \tTrain mse: 0.225 \t Test mse: 0.322\n",
      "Epoch: 82 \tTrain loss: 3900.564 \t Test loss: 5674.660\n",
      "Epoch: 82 \tTrain mse: 0.225 \t Test mse: 0.322\n",
      "Epoch: 83 \tTrain loss: 3902.255 \t Test loss: 5573.859\n",
      "Epoch: 83 \tTrain mse: 0.225 \t Test mse: 0.315\n",
      "Epoch: 84 \tTrain loss: 3871.641 \t Test loss: 5540.295\n",
      "Epoch: 84 \tTrain mse: 0.223 \t Test mse: 0.313\n",
      "Epoch: 85 \tTrain loss: 3888.592 \t Test loss: 5509.273\n",
      "Epoch: 85 \tTrain mse: 0.223 \t Test mse: 0.311\n",
      "Epoch: 86 \tTrain loss: 3886.877 \t Test loss: 5455.628\n",
      "Epoch: 86 \tTrain mse: 0.223 \t Test mse: 0.307\n",
      "Epoch: 87 \tTrain loss: 3873.078 \t Test loss: 5424.275\n",
      "Epoch: 87 \tTrain mse: 0.222 \t Test mse: 0.304\n",
      "Epoch: 88 \tTrain loss: 3854.114 \t Test loss: 5389.769\n",
      "Epoch: 88 \tTrain mse: 0.220 \t Test mse: 0.301\n",
      "Epoch: 89 \tTrain loss: 3840.448 \t Test loss: 5327.137\n",
      "Epoch: 89 \tTrain mse: 0.219 \t Test mse: 0.298\n",
      "Epoch: 90 \tTrain loss: 3830.680 \t Test loss: 5304.942\n",
      "Epoch: 90 \tTrain mse: 0.219 \t Test mse: 0.296\n",
      "Epoch: 91 \tTrain loss: 3829.570 \t Test loss: 5297.158\n",
      "Epoch: 91 \tTrain mse: 0.218 \t Test mse: 0.295\n",
      "Epoch: 92 \tTrain loss: 3805.897 \t Test loss: 5227.761\n",
      "Epoch: 92 \tTrain mse: 0.217 \t Test mse: 0.290\n",
      "Epoch: 93 \tTrain loss: 3799.927 \t Test loss: 5211.135\n",
      "Epoch: 93 \tTrain mse: 0.216 \t Test mse: 0.288\n",
      "Epoch: 94 \tTrain loss: 3794.392 \t Test loss: 5167.121\n",
      "Epoch: 94 \tTrain mse: 0.215 \t Test mse: 0.285\n",
      "Epoch: 95 \tTrain loss: 3788.335 \t Test loss: 5107.958\n",
      "Epoch: 95 \tTrain mse: 0.215 \t Test mse: 0.281\n",
      "Epoch: 96 \tTrain loss: 3787.498 \t Test loss: 5093.772\n",
      "Epoch: 96 \tTrain mse: 0.214 \t Test mse: 0.279\n",
      "Epoch: 97 \tTrain loss: 3785.351 \t Test loss: 5084.168\n",
      "Epoch: 97 \tTrain mse: 0.214 \t Test mse: 0.278\n",
      "Epoch: 98 \tTrain loss: 3778.674 \t Test loss: 5030.144\n",
      "Epoch: 98 \tTrain mse: 0.213 \t Test mse: 0.274\n",
      "Epoch: 99 \tTrain loss: 3758.634 \t Test loss: 5008.135\n",
      "Epoch: 99 \tTrain mse: 0.212 \t Test mse: 0.272\n",
      "Epoch: 100 \tTrain loss: 3761.229 \t Test loss: 4992.653\n",
      "Epoch: 100 \tTrain mse: 0.212 \t Test mse: 0.271\n",
      "Epoch: 101 \tTrain loss: 3748.580 \t Test loss: 4932.443\n",
      "Epoch: 101 \tTrain mse: 0.211 \t Test mse: 0.267\n",
      "Epoch: 102 \tTrain loss: 3723.824 \t Test loss: 4900.696\n",
      "Epoch: 102 \tTrain mse: 0.209 \t Test mse: 0.266\n",
      "Epoch: 103 \tTrain loss: 3724.778 \t Test loss: 4885.002\n",
      "Epoch: 103 \tTrain mse: 0.209 \t Test mse: 0.264\n",
      "Epoch: 104 \tTrain loss: 3724.630 \t Test loss: 4829.298\n",
      "Epoch: 104 \tTrain mse: 0.209 \t Test mse: 0.261\n",
      "Epoch: 105 \tTrain loss: 3730.285 \t Test loss: 4840.993\n",
      "Epoch: 105 \tTrain mse: 0.210 \t Test mse: 0.261\n",
      "Epoch: 106 \tTrain loss: 3699.420 \t Test loss: 4792.345\n",
      "Epoch: 106 \tTrain mse: 0.208 \t Test mse: 0.258\n",
      "Epoch: 107 \tTrain loss: 3716.806 \t Test loss: 4738.363\n",
      "Epoch: 107 \tTrain mse: 0.209 \t Test mse: 0.255\n",
      "Epoch: 108 \tTrain loss: 3690.523 \t Test loss: 4695.154\n",
      "Epoch: 108 \tTrain mse: 0.207 \t Test mse: 0.251\n",
      "Epoch: 109 \tTrain loss: 3683.257 \t Test loss: 4679.326\n",
      "Epoch: 109 \tTrain mse: 0.207 \t Test mse: 0.251\n",
      "Epoch: 110 \tTrain loss: 3674.626 \t Test loss: 4604.382\n",
      "Epoch: 110 \tTrain mse: 0.206 \t Test mse: 0.246\n",
      "Epoch: 111 \tTrain loss: 3672.818 \t Test loss: 4633.955\n",
      "Epoch: 111 \tTrain mse: 0.206 \t Test mse: 0.248\n",
      "Epoch: 112 \tTrain loss: 3669.054 \t Test loss: 4576.780\n",
      "Epoch: 112 \tTrain mse: 0.206 \t Test mse: 0.244\n",
      "Epoch: 113 \tTrain loss: 3665.206 \t Test loss: 4563.734\n",
      "Epoch: 113 \tTrain mse: 0.205 \t Test mse: 0.243\n",
      "Epoch: 114 \tTrain loss: 3642.430 \t Test loss: 4548.163\n",
      "Epoch: 114 \tTrain mse: 0.204 \t Test mse: 0.242\n",
      "Epoch: 115 \tTrain loss: 3634.946 \t Test loss: 4511.216\n",
      "Epoch: 115 \tTrain mse: 0.203 \t Test mse: 0.240\n",
      "Epoch: 116 \tTrain loss: 3634.781 \t Test loss: 4484.111\n",
      "Epoch: 116 \tTrain mse: 0.203 \t Test mse: 0.238\n",
      "Epoch: 117 \tTrain loss: 3612.898 \t Test loss: 4480.595\n",
      "Epoch: 117 \tTrain mse: 0.202 \t Test mse: 0.238\n",
      "Epoch: 118 \tTrain loss: 3624.981 \t Test loss: 4441.353\n",
      "Epoch: 118 \tTrain mse: 0.203 \t Test mse: 0.236\n",
      "Epoch: 119 \tTrain loss: 3608.216 \t Test loss: 4409.634\n",
      "Epoch: 119 \tTrain mse: 0.202 \t Test mse: 0.234\n",
      "Epoch: 120 \tTrain loss: 3609.607 \t Test loss: 4376.297\n",
      "Epoch: 120 \tTrain mse: 0.202 \t Test mse: 0.231\n",
      "Epoch: 121 \tTrain loss: 3611.672 \t Test loss: 4340.193\n",
      "Epoch: 121 \tTrain mse: 0.202 \t Test mse: 0.229\n",
      "Epoch: 122 \tTrain loss: 3582.986 \t Test loss: 4348.751\n",
      "Epoch: 122 \tTrain mse: 0.200 \t Test mse: 0.229\n",
      "Epoch: 123 \tTrain loss: 3592.040 \t Test loss: 4305.319\n",
      "Epoch: 123 \tTrain mse: 0.201 \t Test mse: 0.227\n",
      "Epoch: 124 \tTrain loss: 3578.744 \t Test loss: 4277.034\n",
      "Epoch: 124 \tTrain mse: 0.200 \t Test mse: 0.224\n",
      "Epoch: 125 \tTrain loss: 3562.894 \t Test loss: 4289.427\n",
      "Epoch: 125 \tTrain mse: 0.199 \t Test mse: 0.227\n",
      "Epoch: 126 \tTrain loss: 3571.088 \t Test loss: 4266.940\n",
      "Epoch: 126 \tTrain mse: 0.199 \t Test mse: 0.224\n",
      "Epoch: 127 \tTrain loss: 3550.034 \t Test loss: 4202.279\n",
      "Epoch: 127 \tTrain mse: 0.198 \t Test mse: 0.220\n",
      "Epoch: 128 \tTrain loss: 3557.304 \t Test loss: 4165.273\n",
      "Epoch: 128 \tTrain mse: 0.198 \t Test mse: 0.217\n",
      "Epoch: 129 \tTrain loss: 3545.367 \t Test loss: 4151.964\n",
      "Epoch: 129 \tTrain mse: 0.197 \t Test mse: 0.217\n",
      "Epoch: 130 \tTrain loss: 3535.858 \t Test loss: 4136.578\n",
      "Epoch: 130 \tTrain mse: 0.197 \t Test mse: 0.217\n",
      "Epoch: 131 \tTrain loss: 3528.061 \t Test loss: 4148.669\n",
      "Epoch: 131 \tTrain mse: 0.196 \t Test mse: 0.217\n",
      "Epoch: 132 \tTrain loss: 3526.002 \t Test loss: 4131.866\n",
      "Epoch: 132 \tTrain mse: 0.196 \t Test mse: 0.216\n",
      "Epoch: 133 \tTrain loss: 3527.993 \t Test loss: 4087.549\n",
      "Epoch: 133 \tTrain mse: 0.196 \t Test mse: 0.213\n",
      "Epoch: 134 \tTrain loss: 3525.767 \t Test loss: 4080.493\n",
      "Epoch: 134 \tTrain mse: 0.196 \t Test mse: 0.212\n",
      "Epoch: 135 \tTrain loss: 3513.540 \t Test loss: 4026.944\n",
      "Epoch: 135 \tTrain mse: 0.195 \t Test mse: 0.208\n",
      "Epoch: 136 \tTrain loss: 3498.480 \t Test loss: 4015.257\n",
      "Epoch: 136 \tTrain mse: 0.194 \t Test mse: 0.208\n",
      "Epoch: 137 \tTrain loss: 3506.181 \t Test loss: 4024.149\n",
      "Epoch: 137 \tTrain mse: 0.195 \t Test mse: 0.210\n",
      "Epoch: 138 \tTrain loss: 3497.995 \t Test loss: 3986.417\n",
      "Epoch: 138 \tTrain mse: 0.194 \t Test mse: 0.206\n",
      "Epoch: 139 \tTrain loss: 3507.293 \t Test loss: 3952.228\n",
      "Epoch: 139 \tTrain mse: 0.195 \t Test mse: 0.204\n",
      "Epoch: 140 \tTrain loss: 3499.445 \t Test loss: 3963.946\n",
      "Epoch: 140 \tTrain mse: 0.194 \t Test mse: 0.205\n",
      "Epoch: 141 \tTrain loss: 3484.703 \t Test loss: 3932.684\n",
      "Epoch: 141 \tTrain mse: 0.194 \t Test mse: 0.203\n",
      "Epoch: 142 \tTrain loss: 3498.663 \t Test loss: 3960.815\n",
      "Epoch: 142 \tTrain mse: 0.194 \t Test mse: 0.204\n",
      "Epoch: 143 \tTrain loss: 3460.471 \t Test loss: 3922.775\n",
      "Epoch: 143 \tTrain mse: 0.192 \t Test mse: 0.202\n",
      "Epoch: 144 \tTrain loss: 3472.619 \t Test loss: 3930.670\n",
      "Epoch: 144 \tTrain mse: 0.193 \t Test mse: 0.203\n",
      "Epoch: 145 \tTrain loss: 3443.112 \t Test loss: 3892.164\n",
      "Epoch: 145 \tTrain mse: 0.191 \t Test mse: 0.201\n",
      "Epoch: 146 \tTrain loss: 3454.981 \t Test loss: 3861.016\n",
      "Epoch: 146 \tTrain mse: 0.192 \t Test mse: 0.198\n",
      "Epoch: 147 \tTrain loss: 3464.581 \t Test loss: 3851.539\n",
      "Epoch: 147 \tTrain mse: 0.192 \t Test mse: 0.197\n",
      "Epoch: 148 \tTrain loss: 3445.828 \t Test loss: 3832.940\n",
      "Epoch: 148 \tTrain mse: 0.191 \t Test mse: 0.196\n",
      "Epoch: 149 \tTrain loss: 3448.629 \t Test loss: 3797.218\n",
      "Epoch: 149 \tTrain mse: 0.191 \t Test mse: 0.194\n",
      "Epoch: 150 \tTrain loss: 3438.276 \t Test loss: 3821.036\n",
      "Epoch: 150 \tTrain mse: 0.190 \t Test mse: 0.195\n",
      "Epoch: 151 \tTrain loss: 3456.407 \t Test loss: 3817.877\n",
      "Epoch: 151 \tTrain mse: 0.192 \t Test mse: 0.196\n",
      "Epoch: 152 \tTrain loss: 3435.174 \t Test loss: 3745.832\n",
      "Epoch: 152 \tTrain mse: 0.190 \t Test mse: 0.191\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9f2887ccf246>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain_mse\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_mse = 0\n",
    "    beta = min(1.0, float(epoch) / num_warmup_epochs)\n",
    "    for (data_,) in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data_)\n",
    "        loss = loss_function(recon_batch, data_, mu, logvar, beta)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        train_mse += mse(recon_batch, data_).item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_mse /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_mse = 0\n",
    "    with torch.no_grad():\n",
    "      for (data_,) in test_loader:\n",
    "        recon_batch, mu, logvar = model(data_)\n",
    "        loss = loss_function(recon_batch, data_, mu, logvar, beta)\n",
    "        test_loss += loss.item()\n",
    "        test_mse += mse(recon_batch, data_).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_mse /= len(test_loader)\n",
    "\n",
    "    print('Epoch: {} \\tTrain loss: {:.3f} \\t Test loss: {:.3f}'.format(epoch, train_loss, test_loss))\n",
    "    print('Epoch: {} \\tTrain mse: {:.3f} \\t Test mse: {:.3f}'.format(epoch, train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "drtPl_e1Ew-p"
   },
   "outputs": [],
   "source": [
    "# save model state dict to file\n",
    "torch.save(model.state_dict(), 'vae_300_epochs.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
